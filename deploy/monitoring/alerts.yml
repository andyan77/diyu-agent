# DIYU Agent Prometheus Alert Rules
# Task cards: OS2-2 (basic alerts), OS2-3 (token anomaly)
#
# Alert levels: P0-Critical, P1-Warning, P2-Info

groups:
  - name: diyu_golden_signals
    rules:
      # OS2-2: Error rate > 1% over 5 minutes
      - alert: HighErrorRate
        expr: |
          sum(rate(http_errors_total[5m]))
          / sum(rate(http_requests_total[5m]))
          > 0.01
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "HTTP error rate above 1%"
          description: >
            Error rate is {{ $value | humanizePercentage }} over the last 5 minutes.
            Threshold: 1%.

      # OS2-2: P95 latency > 2s over 5 minutes
      - alert: HighP95Latency
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
          > 2
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "P95 latency above 2 seconds"
          description: >
            P95 request latency is {{ $value | humanizeDuration }}.
            Threshold: 2 seconds.

      # OS2-2: Service down (no traffic for 5 minutes)
      - alert: ServiceDown
        expr: |
          sum(rate(http_requests_total[5m])) == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "No HTTP traffic detected"
          description: "No requests received in the last 5 minutes."

  - name: diyu_token_alerts
    rules:
      # OS2-3: Token consumption anomaly - single conversation exceeds threshold
      - alert: TokenConsumptionAnomaly
        expr: |
          max(llm_tokens_per_conversation) > 50000
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Abnormal token consumption detected"
          description: >
            A single conversation consumed {{ $value }} tokens.
            Threshold: 50,000 tokens.

      # OS2-3: Aggregate token rate anomaly
      - alert: TokenRateAnomaly
        expr: |
          sum(rate(llm_tokens_total[10m])) > 100000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High aggregate token consumption rate"
          description: >
            Token consumption rate is {{ $value }} tokens/s over 10 minutes.
            Threshold: 100,000 tokens/10min.

  # Phase 4: SLO burn-rate alerts (Google SRE Workbook, decision C-3/C-5)
  # SLO target: 99.5% availability, 30-day window, error budget = 216 min/month
  - name: diyu_slo_burn_rate
    rules:
      # Fast burn: 14.4x rate over 1h window -> P0 page
      # At 14.4x burn, entire monthly budget consumed in ~15 hours
      - alert: SLOBurnRateFast
        expr: |
          (
            sum(rate(http_errors_total[1h]))
            / sum(rate(http_requests_total[1h]))
          ) > (14.4 * 0.005)
        for: 2m
        labels:
          severity: critical
          slo: "availability"
          burn_rate: "fast"
        annotations:
          summary: "SLO fast burn: error budget depleting rapidly (14.4x)"
          description: >
            Error rate is {{ $value | humanizePercentage }} over the last 1 hour,
            which is 14.4x the SLO error budget rate.
            At this rate, monthly error budget (216 min) will be exhausted in ~15 hours.
            Action: Page founder immediately.

      # Slow burn: 6x rate over 6h window -> P1 alert
      # At 6x burn, entire monthly budget consumed in ~36 hours
      - alert: SLOBurnRateSlow
        expr: |
          (
            sum(rate(http_errors_total[6h]))
            / sum(rate(http_requests_total[6h]))
          ) > (6 * 0.005)
        for: 5m
        labels:
          severity: warning
          slo: "availability"
          burn_rate: "slow"
        annotations:
          summary: "SLO slow burn: error budget depleting steadily (6x)"
          description: >
            Error rate is {{ $value | humanizePercentage }} over the last 6 hours,
            which is 6x the SLO error budget rate.
            At this rate, monthly error budget (216 min) will be exhausted in ~36 hours.
            Action: Investigate within 1 hour.

      # Latency SLO burn: P95 latency exceeding 500ms target
      - alert: SLOLatencyBurn
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[1h])) by (le))
          > 0.5
        for: 5m
        labels:
          severity: warning
          slo: "latency"
          burn_rate: "sustained"
        annotations:
          summary: "SLO latency burn: P95 latency exceeds 500ms target"
          description: >
            P95 request latency is {{ $value | humanizeDuration }} over the last hour.
            SLO target: 500ms. Sustained degradation will erode user experience.
